{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "J-Final datamining.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLtnuVkqdSf1",
        "colab_type": "text"
      },
      "source": [
        "# Downloads"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzGCotJAdRIM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjP9Cl64VTKN",
        "colab_type": "text"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6G0z-4nVTft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "from tensorflow.python.keras import backend as keras\n",
        "from tensorflow.python.keras.layers import *\n",
        "from tensorflow.python.keras.models import *\n",
        "from tensorflow.python.keras.optimizers import *\n",
        "from google.colab import files\n",
        "import os\n",
        "from os.path import join as pjoin\n",
        "import collections\n",
        "import json\n",
        "import torch\n",
        "import numpy as np\n",
        "import scipy.misc as m\n",
        "import scipy.io as io\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import tensorflow\n",
        "from numpy import asarray\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import logging\n",
        "from torch.optim import SGD, Adam, ASGD, Adamax, Adadelta, Adagrad, RMSprop\n",
        "import torch.optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR, ExponentialLR, CosineAnnealingLR\n",
        "import torch.nn.functional as F\n",
        "import functools\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "from torch.utils import data\n",
        "from torchvision import transforms\n",
        "\n",
        "disable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqIbFmtVd09",
        "colab_type": "text"
      },
      "source": [
        "# **Get** **the** **Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKUCgC8kVeKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n",
        "!tar -xf VOCtrainval_11-May-2012.tar\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpTBQOXQVoJc",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vAry0AbVOun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pascalVOCLoader(data.Dataset):\n",
        "    \"\"\"Data loader for the Pascal VOC semantic segmentation dataset.\n",
        "    Annotations from both the original VOC data (which consist of RGB images\n",
        "    in which colours map to specific classes) and the SBD (Berkely) dataset\n",
        "    (where annotations are stored as .mat files) are converted into a common\n",
        "    `label_mask` format.  Under this format, each mask is an (M,N) array of\n",
        "    integer values from 0 to 21, where 0 represents the background class.\n",
        "    The label masks are stored in a new folder, called `pre_encoded`, which\n",
        "    is added as a subdirectory of the `SegmentationClass` folder in the\n",
        "    original Pascal VOC data layout.\n",
        "    A total of five data splits are provided for working with the VOC data:\n",
        "        train: The original VOC 2012 training data - 1464 images\n",
        "        val: The original VOC 2012 validation data - 1449 images\n",
        "        trainval: The combination of `train` and `val` - 2913 images\n",
        "        train_aug: The unique images present in both the train split and\n",
        "                   training images from SBD: - 8829 images (the unique members\n",
        "                   of the result of combining lists of length 1464 and 8498)\n",
        "        train_aug_val: The original VOC 2012 validation data minus the images\n",
        "                   present in `train_aug` (This is done with the same logic as\n",
        "                   the validation set used in FCN PAMI paper, but with VOC 2012\n",
        "                   rather than VOC 2011) - 904 images\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,root,sbd_path=None,split=\"train_aug\",is_transform=False,img_size=512,\n",
        "    augmentations=None,img_norm=True,\n",
        "        test_mode=False,):\n",
        "        self.root = root\n",
        "        self.sbd_path = sbd_path\n",
        "        self.split = split\n",
        "        self.is_transform = is_transform\n",
        "        self.augmentations = augmentations\n",
        "        self.img_norm = img_norm\n",
        "        self.test_mode = test_mode\n",
        "        self.n_classes = 21\n",
        "        self.mean = np.array([104.00699, 116.66877, 122.67892])\n",
        "        self.files = collections.defaultdict(list)\n",
        "        self.img_size = img_size if isinstance(img_size, tuple) else (img_size, img_size)\n",
        "\n",
        "        if not self.test_mode:\n",
        "            for split in [\"train\", \"val\", \"trainval\"]:\n",
        "                path = pjoin(self.root, \"ImageSets/Segmentation\", split + \".txt\")\n",
        "                file_list = tuple(open(path, \"r\"))\n",
        "                file_list = [id_.rstrip() for id_ in file_list]\n",
        "                self.files[split] = file_list\n",
        "            #self.setup_annotations()\n",
        "        self.tf = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),  \n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.files[self.split])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      im_name = self.files[self.split][index]\n",
        "      im_path = pjoin(self.root, \"JPEGImages\", im_name + \".jpg\")\n",
        "      lbl_path = pjoin(self.root, \"SegmentationClass/\", im_name + \".png\")\n",
        "    #   lbl_path = pjoin(self.root, \"SegmentationClass/pre_encoded\", im_name + \".png\")\n",
        "      im = Image.open(im_path)\n",
        "      lbl = Image.open(lbl_path)\n",
        "      if self.augmentations is not None:\n",
        "        im, lbl = self.augmentations(im, lbl)\n",
        "      if self.is_transform:\n",
        "        im, lbl = self.transform(im, lbl)\n",
        "      return im, lbl\n",
        "\n",
        "    def transform(self, img, lbl):\n",
        "        if self.img_size == (\"same\", \"same\"):\n",
        "            pass\n",
        "        else:\n",
        "            # img = np.array(img) \n",
        "            # lbl = np.array(lbl) \n",
        "            img = img.resize((self.img_size[0], self.img_size[1]))  # uint8 with RGB mode\n",
        "            lbl = lbl.resize((self.img_size[0], self.img_size[1]))\n",
        "            # img = tf.image.convert_image_dtype(img, tf.float64, saturate=False, name=None)\n",
        "            # lbl = tf.image.convert_image_dtype(lbl, tf.float64, saturate=False, name=None)\n",
        "\n",
        "        # print(\"IMG:\" + str(type(img)))\n",
        "        # print(\"LBL:\" + str(type(lbl)))\n",
        "        img = self.tf(img)\n",
        "        # img = img.numpy()\n",
        "        # img = asarray(img)\n",
        "        # img = tensorflow.keras.preprocessing.image.img_to_array(img, data_format=None, dtype=tensorflow.float64)\n",
        "        lbl = torch.from_numpy(np.array(lbl)).long()\n",
        "        lbl[lbl == 255] = 0\n",
        "        # lbl = asarray(lbl)\n",
        "        # print(\"IMG:\" + str(type(img)))\n",
        "        # print(\"LBL:\" + str(type(lbl)))\n",
        "        return img, lbl\n",
        "\n",
        "    def get_pascal_labels(self):\n",
        "        \"\"\"Load the mapping that associates pascal classes with label colors\n",
        "        Returns:\n",
        "            np.ndarray with dimensions (21, 3)\n",
        "        \"\"\"\n",
        "        return np.asarray(\n",
        "            [\n",
        "                [0, 0, 0],\n",
        "                [128, 0, 0],\n",
        "                [0, 128, 0],\n",
        "                [128, 128, 0],\n",
        "                [0, 0, 128],\n",
        "                [128, 0, 128],\n",
        "                [0, 128, 128],\n",
        "                [128, 128, 128],\n",
        "                [64, 0, 0],\n",
        "                [192, 0, 0],\n",
        "                [64, 128, 0],\n",
        "                [192, 128, 0],\n",
        "                [64, 0, 128],\n",
        "                [192, 0, 128],\n",
        "                [64, 128, 128],\n",
        "                [192, 128, 128],\n",
        "                [0, 64, 0],\n",
        "                [128, 64, 0],\n",
        "                [0, 192, 0],\n",
        "                [128, 192, 0],\n",
        "                [0, 64, 128],\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    def encode_segmap(self, mask):\n",
        "        \"\"\"Encode segmentation label images as pascal classes\n",
        "        Args:\n",
        "            mask (np.ndarray): raw segmentation label image of dimension\n",
        "              (M, N, 3), in which the Pascal classes are encoded as colours.\n",
        "        Returns:\n",
        "            (np.ndarray): class map with dimensions (M,N), where the value at\n",
        "            a given location is the integer denoting the class index.\n",
        "        \"\"\"\n",
        "        mask = mask.astype(int)\n",
        "        label_mask = np.zeros((mask.shape[0], mask.shape[1]), dtype=np.int16)\n",
        "        for ii, label in enumerate(self.get_pascal_labels()):\n",
        "            label_mask[np.where(np.all(mask == label, axis=-1))[:2]] = ii\n",
        "        label_mask = label_mask.astype(int)\n",
        "        return label_mask\n",
        "\n",
        "    def decode_segmap(self, label_mask, plot=False):\n",
        "        \"\"\"Decode segmentation class labels into a color image\n",
        "        Args:\n",
        "            label_mask (np.ndarray): an (M,N) array of integer values denoting\n",
        "              the class label at each spatial location.\n",
        "            plot (bool, optional): whether to show the resulting color image\n",
        "              in a figure.\n",
        "        Returns:\n",
        "            (np.ndarray, optional): the resulting decoded color image.\n",
        "        \"\"\"\n",
        "        label_colours = self.get_pascal_labels()\n",
        "        r = label_mask.copy()\n",
        "        g = label_mask.copy()\n",
        "        b = label_mask.copy()\n",
        "        for ll in range(0, self.n_classes):\n",
        "            r[label_mask == ll] = label_colours[ll, 0]\n",
        "            g[label_mask == ll] = label_colours[ll, 1]\n",
        "            b[label_mask == ll] = label_colours[ll, 2]\n",
        "        rgb = np.zeros((label_mask.shape[0], label_mask.shape[1], 3))\n",
        "        rgb[:, :, 0] = r / 255.0\n",
        "        rgb[:, :, 1] = g / 255.0\n",
        "        rgb[:, :, 2] = b / 255.0\n",
        "        if plot:\n",
        "            plt.imshow(rgb)\n",
        "            plt.show()\n",
        "        else:\n",
        "            return rgb\n",
        "\n",
        "    def setup_annotations(self):\n",
        "        \"\"\"Sets up Berkley annotations by adding image indices to the\n",
        "        `train_aug` split and pre-encode all segmentation labels into the\n",
        "        common label_mask format (if this has not already been done). This\n",
        "        function also defines the `train_aug` and `train_aug_val` data splits\n",
        "        according to the description in the class docstring\n",
        "        \"\"\"\n",
        "        sbd_path = self.sbd_path\n",
        "        target_path = pjoin(self.root, \"SegmentationClass/pre_encoded\")\n",
        "        if not os.path.exists(target_path):\n",
        "            os.makedirs(target_path)\n",
        "        path = pjoin(sbd_path, \"dataset/train.txt\")\n",
        "        sbd_train_list = tuple(open(path, \"r\"))\n",
        "        sbd_train_list = [id_.rstrip() for id_ in sbd_train_list]\n",
        "        train_aug = self.files[\"train\"] + sbd_train_list\n",
        "\n",
        "        # keep unique elements (stable)\n",
        "        train_aug = [train_aug[i] for i in sorted(np.unique(train_aug, return_index=True)[1])]\n",
        "        self.files[\"train_aug\"] = train_aug\n",
        "        set_diff = set(self.files[\"val\"]) - set(train_aug)  # remove overlap\n",
        "        self.files[\"train_aug_val\"] = list(set_diff)\n",
        "\n",
        "        pre_encoded = glob.glob(pjoin(target_path, \"*.png\"))\n",
        "        expected = np.unique(self.files[\"train_aug\"] + self.files[\"val\"]).size\n",
        "\n",
        "        if len(pre_encoded) != expected:\n",
        "            print(\"Pre-encoding segmentation masks...\")\n",
        "            for ii in tqdm(sbd_train_list):\n",
        "                lbl_path = pjoin(sbd_path, \"dataset/cls\", ii + \".mat\")\n",
        "                data = io.loadmat(lbl_path)\n",
        "                lbl = data[\"GTcls\"][0][\"Segmentation\"][0].astype(np.int32)\n",
        "                lbl = m.toimage(lbl, high=lbl.max(), low=lbl.min())\n",
        "                m.imsave(pjoin(target_path, ii + \".png\"), lbl)\n",
        "\n",
        "            for ii in tqdm(self.files[\"trainval\"]):\n",
        "                fname = ii + \".png\"\n",
        "                lbl_path = pjoin(self.root, \"SegmentationClass\", fname)\n",
        "                lbl = self.encode_segmap(m.imread(lbl_path))\n",
        "                lbl = m.toimage(lbl, high=lbl.max(), low=lbl.min())\n",
        "                m.imsave(pjoin(target_path, fname), lbl)\n",
        "\n",
        "        assert expected == 9733, \"unexpected dataset sizes\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Fs5gjiXVtYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, augmentations):\n",
        "        self.augmentations = augmentations\n",
        "        self.PIL2Numpy = False\n",
        "\n",
        "    def __call__(self, img, mask):\n",
        "        if isinstance(img, np.ndarray):\n",
        "            img = Image.fromarray(img, mode=\"RGB\")\n",
        "            mask = Image.fromarray(mask, mode=\"L\")\n",
        "            self.PIL2Numpy = True\n",
        "\n",
        "        assert img.size == mask.size\n",
        "        for a in self.augmentations:\n",
        "            img, mask = a(img, mask)\n",
        "\n",
        "        if self.PIL2Numpy:\n",
        "            img, mask = np.array(img), np.array(mask, dtype=np.uint8)\n",
        "\n",
        "        return img, mask\n",
        "\n",
        "\n",
        "class RandomHorizontallyFlip(object):\n",
        "    def __init__(self, p):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img, mask):\n",
        "        if random.random() < self.p:\n",
        "            return (img.transpose(Image.FLIP_LEFT_RIGHT), mask.transpose(Image.FLIP_LEFT_RIGHT))\n",
        "        return img, mask\n",
        "\n",
        "\n",
        "class RandomRotate(object):\n",
        "    def __init__(self, degree):\n",
        "        self.degree = degree\n",
        "\n",
        "    def __call__(self, img, mask):\n",
        "        rotate_degree = random.random() * 2 * self.degree - self.degree\n",
        "        return (\n",
        "            tf.affine(\n",
        "                img,\n",
        "                translate=(0, 0),\n",
        "                scale=1.0,\n",
        "                angle=rotate_degree,\n",
        "                resample=Image.BILINEAR,\n",
        "                fillcolor=(0, 0, 0),\n",
        "                shear=0.0,\n",
        "            ),\n",
        "            tf.affine(\n",
        "                mask,\n",
        "                translate=(0, 0),\n",
        "                scale=1.0,\n",
        "                angle=rotate_degree,\n",
        "                resample=Image.NEAREST,\n",
        "                fillcolor=250,\n",
        "                shear=0.0,\n",
        "            ),\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p9M9ylEWCW9",
        "colab_type": "text"
      },
      "source": [
        "# **Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1h_ZGG3VwtJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class fcn8s(nn.Module):\n",
        "    def __init__(self, n_classes=21, learned_billinear=True):\n",
        "        super(fcn8s, self).__init__()\n",
        "        self.learned_billinear = learned_billinear\n",
        "        self.n_classes = n_classes\n",
        "        self.loss = functools.partial(cross_entropy2d, size_average=False)\n",
        "\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=100),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        self.conv_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        self.conv_block5 = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(512, 4096, 7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(),\n",
        "            nn.Conv2d(4096, 4096, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(),\n",
        "            nn.Conv2d(4096, self.n_classes, 1),\n",
        "        )\n",
        "\n",
        "        self.score_pool4 = nn.Conv2d(512, self.n_classes, 1)\n",
        "        self.score_pool3 = nn.Conv2d(256, self.n_classes, 1)\n",
        "\n",
        "        if self.learned_billinear:\n",
        "            self.upscore2 = nn.ConvTranspose2d(\n",
        "                self.n_classes, self.n_classes, 4, stride=2, bias=False\n",
        "            )\n",
        "            self.upscore4 = nn.ConvTranspose2d(\n",
        "                self.n_classes, self.n_classes, 4, stride=2, bias=False\n",
        "            )\n",
        "            self.upscore8 = nn.ConvTranspose2d(\n",
        "                self.n_classes, self.n_classes, 16, stride=8, bias=False\n",
        "            )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.ConvTranspose2d):\n",
        "                m.weight.data.copy_(\n",
        "                    get_upsampling_weight(m.in_channels, m.out_channels, m.kernel_size[0])\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv_block1(x)\n",
        "        conv2 = self.conv_block2(conv1)\n",
        "        conv3 = self.conv_block3(conv2)\n",
        "        conv4 = self.conv_block4(conv3)\n",
        "        conv5 = self.conv_block5(conv4)\n",
        "\n",
        "        score = self.classifier(conv5)\n",
        "\n",
        "        if self.learned_billinear:\n",
        "            upscore2 = self.upscore2(score)\n",
        "            score_pool4c = self.score_pool4(conv4)[\n",
        "                :, :, 5 : 5 + upscore2.size()[2], 5 : 5 + upscore2.size()[3]\n",
        "            ]\n",
        "            upscore_pool4 = self.upscore4(upscore2 + score_pool4c)\n",
        "\n",
        "            score_pool3c = self.score_pool3(conv3)[\n",
        "                :, :, 9 : 9 + upscore_pool4.size()[2], 9 : 9 + upscore_pool4.size()[3]\n",
        "            ]\n",
        "\n",
        "            out = self.upscore8(score_pool3c + upscore_pool4)[\n",
        "                :, :, 31 : 31 + x.size()[2], 31 : 31 + x.size()[3]\n",
        "            ]\n",
        "            return out.contiguous()\n",
        "\n",
        "        else:\n",
        "            score_pool4 = self.score_pool4(conv4)\n",
        "            score_pool3 = self.score_pool3(conv3)\n",
        "            score = F.upsample(score, score_pool4.size()[2:])\n",
        "            score += score_pool4\n",
        "            score = F.upsample(score, score_pool3.size()[2:])\n",
        "            score += score_pool3\n",
        "            out = F.upsample(score, x.size()[2:])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def init_vgg16_params(self, vgg16, copy_fc8=True):\n",
        "        blocks = [\n",
        "            self.conv_block1,\n",
        "            self.conv_block2,\n",
        "            self.conv_block3,\n",
        "            self.conv_block4,\n",
        "            self.conv_block5,\n",
        "        ]\n",
        "\n",
        "        ranges = [[0, 4], [5, 9], [10, 16], [17, 23], [24, 29]]\n",
        "        features = list(vgg16.features.children())\n",
        "\n",
        "        for idx, conv_block in enumerate(blocks):\n",
        "            for l1, l2 in zip(features[ranges[idx][0] : ranges[idx][1]], conv_block):\n",
        "                if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
        "                    assert l1.weight.size() == l2.weight.size()\n",
        "                    assert l1.bias.size() == l2.bias.size()\n",
        "                    l2.weight.data = l1.weight.data\n",
        "                    l2.bias.data = l1.bias.data\n",
        "        for i1, i2 in zip([0, 3], [0, 3]):\n",
        "            l1 = vgg16.classifier[i1]\n",
        "            l2 = self.classifier[i2]\n",
        "            l2.weight.data = l1.weight.data.view(l2.weight.size())\n",
        "            l2.bias.data = l1.bias.data.view(l2.bias.size())\n",
        "        n_class = self.classifier[6].weight.size()[0]\n",
        "        if copy_fc8:\n",
        "            l1 = vgg16.classifier[6]\n",
        "            l2 = self.classifier[6]\n",
        "            l2.weight.data = l1.weight.data[:n_class, :].view(l2.weight.size())\n",
        "            l2.bias.data = l1.bias.data[:n_class]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0Ybaed-WBdp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n",
        "    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n",
        "    factor = (kernel_size + 1) // 2\n",
        "    if kernel_size % 2 == 1:\n",
        "        center = factor - 1\n",
        "    else:\n",
        "        center = factor - 0.5\n",
        "    og = np.ogrid[:kernel_size, :kernel_size]\n",
        "    filt = (1 - abs(og[0] - center) / factor) * (1 - abs(og[1] - center) / factor)\n",
        "    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size), dtype=np.float64)\n",
        "    weight[range(in_channels), range(out_channels), :, :] = filt\n",
        "    return torch.from_numpy(weight).float()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDdCV5cGaKD8",
        "colab_type": "text"
      },
      "source": [
        "## Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygTfh1XCaLfe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_entropy2d(input, target, weight=None, size_average=True):\n",
        "    n, c, h, w = input.size()\n",
        "    nt, ht, wt = target.size()\n",
        "\n",
        "    # Handle inconsistent size between input and target\n",
        "    if h != ht and w != wt:  # upsample labels\n",
        "        input = F.interpolate(input, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "    input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
        "    target = target.view(-1)\n",
        "    loss = F.cross_entropy(\n",
        "        input, target, weight=weight, size_average=size_average, ignore_index=250\n",
        "    )\n",
        "    return loss\n",
        "\n",
        "\n",
        "def multi_scale_cross_entropy2d(input, target, weight=None, size_average=True, scale_weight=None):\n",
        "    if not isinstance(input, tuple):\n",
        "        return cross_entropy2d(input=input, target=target, weight=weight, size_average=size_average)\n",
        "\n",
        "    # Auxiliary training for PSPNet [1.0, 0.4] and ICNet [1.0, 0.4, 0.16]\n",
        "    if scale_weight is None:  # scale_weight: torch tensor type\n",
        "        n_inp = len(input)\n",
        "        scale = 0.4\n",
        "        scale_weight = torch.pow(scale * torch.ones(n_inp), torch.arange(n_inp).float()).to(\n",
        "            target.device\n",
        "        )\n",
        "\n",
        "    loss = 0.0\n",
        "    for i, inp in enumerate(input):\n",
        "        loss = loss + scale_weight[i] * cross_entropy2d(\n",
        "            input=inp, target=target, weight=weight, size_average=size_average\n",
        "        )\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "def bootstrapped_cross_entropy2d(input, target, K, weight=None, size_average=True):\n",
        "\n",
        "    batch_size = input.size()[0]\n",
        "\n",
        "    def _bootstrap_xentropy_single(input, target, K, weight=None, size_average=True):\n",
        "\n",
        "        n, c, h, w = input.size()\n",
        "        input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
        "        target = target.view(-1)\n",
        "        loss = F.cross_entropy(\n",
        "            input, target, weight=weight, reduce=False, size_average=False, ignore_index=250\n",
        "        )\n",
        "\n",
        "        topk_loss, _ = loss.topk(K)\n",
        "        reduced_topk_loss = topk_loss.sum() / K\n",
        "\n",
        "        return reduced_topk_loss\n",
        "\n",
        "    loss = 0.0\n",
        "    # Bootstrap from each image not entire batch\n",
        "    for i in range(batch_size):\n",
        "        loss += _bootstrap_xentropy_single(\n",
        "            input=torch.unsqueeze(input[i], 0),\n",
        "            target=torch.unsqueeze(target[i], 0),\n",
        "            K=K,\n",
        "            weight=weight,\n",
        "            size_average=size_average,\n",
        "        )\n",
        "    return loss / float(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM8ID7AzaO0g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(\"ptsemseg\")\n",
        "\n",
        "lossdict = {\n",
        "    \"cross_entropy\": cross_entropy2d,\n",
        "    \"bootstrapped_cross_entropy\": bootstrapped_cross_entropy2d,\n",
        "    \"multi_scale_cross_entropy\": multi_scale_cross_entropy2d,\n",
        "}\n",
        "\n",
        "\n",
        "def get_loss_function(loss_name, size_average):\n",
        "    if loss_name is None:\n",
        "        logger.info(\"Using default cross entropy loss\")\n",
        "        return cross_entropy2d\n",
        "\n",
        "    else:\n",
        "        loss_params = {\"size_average\": size_average}\n",
        "\n",
        "        if loss_name not in lossdict:\n",
        "            raise NotImplementedError(\"Loss {} not implemented\".format(loss_name))\n",
        "\n",
        "        logger.info(\"Using {} with {} params\".format(loss_name, loss_params))\n",
        "        return functools.partial(lossdict[loss_name], **loss_params)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPf--Fc2WVMt",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Efw3YCq1WQAR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logger = logging.getLogger(\"ptsemseg\")\n",
        "\n",
        "\n",
        "def get_optimizer(model, name, lr, weight_decay):\n",
        "    optdict = {\n",
        "    \"sgd\": SGD,\n",
        "    \"adam\": Adam,\n",
        "    \"asgd\": ASGD,\n",
        "    \"adamax\": Adamax,\n",
        "    \"adadelta\": Adadelta,\n",
        "    \"adagrad\": Adagrad,\n",
        "    \"rmsprop\": RMSprop,\n",
        "    }\n",
        "\n",
        "    if name not in optdict:\n",
        "        raise NotImplementedError(\"Optimizer {} not implemented\".format(name))\n",
        "\n",
        "    logger.info(\"Using {} optimizer\".format(name))\n",
        "\n",
        "    return optdict[name](model.parameters(), lr=lr, weight_decay=weight_decay)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W_Ff21EXWte",
        "colab_type": "text"
      },
      "source": [
        "## Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_60V7tMW75P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ConstantLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, last_epoch=-1):\n",
        "        super(ConstantLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        return [base_lr for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class PolynomialLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(self, optimizer, max_iter, decay_iter=1, gamma=0.9, last_epoch=-1):\n",
        "        self.decay_iter = decay_iter\n",
        "        self.max_iter = max_iter\n",
        "        self.gamma = gamma\n",
        "        super(PolynomialLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch % self.decay_iter or self.last_epoch % self.max_iter:\n",
        "            return [base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            factor = (1 - self.last_epoch / float(self.max_iter)) ** self.gamma\n",
        "            return [base_lr * factor for base_lr in self.base_lrs]\n",
        "\n",
        "\n",
        "class WarmUpLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    def __init__(\n",
        "        self, optimizer, scheduler, mode=\"linear\", warmup_iters=100, gamma=0.2, last_epoch=-1\n",
        "    ):\n",
        "        self.mode = mode\n",
        "        self.scheduler = scheduler\n",
        "        self.warmup_iters = warmup_iters\n",
        "        self.gamma = gamma\n",
        "        super(WarmUpLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        cold_lrs = self.scheduler.get_lr()\n",
        "\n",
        "        if self.last_epoch < self.warmup_iters:\n",
        "            if self.mode == \"linear\":\n",
        "                alpha = self.last_epoch / float(self.warmup_iters)\n",
        "                factor = self.gamma * (1 - alpha) + alpha\n",
        "\n",
        "            elif self.mode == \"constant\":\n",
        "                factor = self.gamma\n",
        "            else:\n",
        "                raise KeyError(\"WarmUp type {} not implemented\".format(self.mode))\n",
        "\n",
        "            return [factor * base_lr for base_lr in cold_lrs]\n",
        "\n",
        "        return cold_lrs\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Bch7z21XjlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "schedulerdict = {\n",
        "    \"constant_lr\": ConstantLR,\n",
        "    \"poly_lr\": PolynomialLR,\n",
        "    \"multi_step\": MultiStepLR,\n",
        "    \"cosine_annealing\": CosineAnnealingLR,\n",
        "    \"exp_lr\": ExponentialLR,\n",
        "}\n",
        "\n",
        "\n",
        "def get_scheduler(optimizer, scheduler_dict):\n",
        "    if scheduler_dict is None:\n",
        "        logger.info(\"Using No LR Scheduling\")\n",
        "        return ConstantLR(optimizer)\n",
        "\n",
        "    s_type = scheduler_dict[\"name\"]\n",
        "    scheduler_dict.pop(\"name\")\n",
        "\n",
        "    logging.info(\"Using {} scheduler with {} params\".format(s_type, scheduler_dict))\n",
        "\n",
        "    warmup_dict = {}\n",
        "    if \"warmup_iters\" in scheduler_dict:\n",
        "        # This can be done in a more pythonic way...\n",
        "        warmup_dict[\"warmup_iters\"] = scheduler_dict.get(\"warmup_iters\", 100)\n",
        "        warmup_dict[\"mode\"] = scheduler_dict.get(\"warmup_mode\", \"linear\")\n",
        "        warmup_dict[\"gamma\"] = scheduler_dict.get(\"warmup_factor\", 0.2)\n",
        "\n",
        "        logger.info(\n",
        "            \"Using Warmup with {} iters {} gamma and {} mode\".format(\n",
        "                warmup_dict[\"warmup_iters\"], warmup_dict[\"gamma\"], warmup_dict[\"mode\"]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        scheduler_dict.pop(\"warmup_iters\", None)\n",
        "        scheduler_dict.pop(\"warmup_mode\", None)\n",
        "        scheduler_dict.pop(\"warmup_factor\", None)\n",
        "\n",
        "        base_scheduler = schedulerdict[s_type](optimizer, **scheduler_dict)\n",
        "        return WarmUpLR(optimizer, base_scheduler, **warmup_dict)\n",
        "\n",
        "    return schedulerdict[s_type](optimizer, **scheduler_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTZqZ5qAcv32",
        "colab_type": "text"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrgagtnrXv5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class runningScore(object):\n",
        "    def __init__(self, n_classes):\n",
        "        self.n_classes = n_classes\n",
        "        self.confusion_matrix = np.zeros((n_classes, n_classes))\n",
        "\n",
        "    def _fast_hist(self, label_true, label_pred, n_class):\n",
        "        mask = (label_true >= 0) & (label_true < n_class)\n",
        "        hist = np.bincount(\n",
        "            n_class * label_true[mask].astype(int) + label_pred[mask], minlength=n_class ** 2\n",
        "        ).reshape(n_class, n_class)\n",
        "        return hist\n",
        "\n",
        "    def update(self, label_trues, label_preds):\n",
        "        for lt, lp in zip(label_trues, label_preds):\n",
        "            self.confusion_matrix += self._fast_hist(lt.flatten(), lp.flatten(), self.n_classes)\n",
        "\n",
        "    def get_scores(self):\n",
        "        \"\"\"Returns accuracy score evaluation result.\n",
        "            - overall accuracy\n",
        "            - mean accuracy\n",
        "            - mean IU\n",
        "            - fwavacc\n",
        "        \"\"\"\n",
        "        hist = self.confusion_matrix\n",
        "        acc = np.diag(hist).sum() / hist.sum()\n",
        "        acc_cls = np.diag(hist) / hist.sum(axis=1)\n",
        "        acc_cls = np.nanmean(acc_cls)\n",
        "        iu = np.diag(hist) / (hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist))\n",
        "        mean_iu = np.nanmean(iu)\n",
        "        freq = hist.sum(axis=1) / hist.sum()\n",
        "        fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n",
        "        cls_iu = dict(zip(range(self.n_classes), iu))\n",
        "\n",
        "        return (\n",
        "            {\n",
        "                \"Overall Acc: \\t\": acc,\n",
        "                \"Mean Acc : \\t\": acc_cls,\n",
        "                \"FreqW Acc : \\t\": fwavacc,\n",
        "                \"Mean IoU : \\t\": mean_iu,\n",
        "            },\n",
        "            cls_iu,\n",
        "        )\n",
        "\n",
        "    def reset(self):\n",
        "        self.confusion_matrix = np.zeros((self.n_classes, self.n_classes))\n",
        "\n",
        "class averageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z33Viie4fiW9",
        "colab_type": "text"
      },
      "source": [
        "## Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Mwp_M_fkHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loader(name):\n",
        "    \"\"\"get_loader\n",
        "    :param name:\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"pascal\": pascalVOCLoader,\n",
        "    }[name]\n",
        "\n",
        "logger = logging.getLogger(\"ptsemseg\")\n",
        "\n",
        "def get_composed_augmentations(aug_dict):\n",
        "    if aug_dict is None:\n",
        "        logger.info(\"Using No Augmentations\")\n",
        "        return None\n",
        "\n",
        "    augmentations = []\n",
        "    for aug_key, aug_param in aug_dict.items():\n",
        "        augmentations.append(key2aug[aug_key](aug_param))\n",
        "        logger.info(\"Using {} aug with params {}\".format(aug_key, aug_param))\n",
        "    return Compose(augmentations)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5LE5tr4eFzo",
        "colab_type": "text"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MggtBBiLeIBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setupLoaders(path, img_size, batch_size, num_workers):\n",
        "    data_loader = get_loader(\"pascal\")\n",
        "\n",
        "    t_loader = data_loader(\n",
        "            path,\n",
        "            is_transform=True,\n",
        "            split=\"train\",\n",
        "            img_size=(512, 512),\n",
        "        )\n",
        "    print(\"T-Loader:\" +  str(len(t_loader)))\n",
        "\n",
        "    trainloader = data.DataLoader(\n",
        "            t_loader,\n",
        "            batch_size=1,\n",
        "            num_workers=16,\n",
        "            shuffle=True,\n",
        "        )\n",
        "     \n",
        "    print(\"Train-Loader:\" +  str(len(trainloader)))\n",
        "     \n",
        "    v_loader = pascalVOCLoader(\n",
        "        root=path,\n",
        "        is_transform=True,\n",
        "        split=\"val\",\n",
        "        img_size=(512, 512),\n",
        "    )\n",
        "    print(\"V-Loader:\" +  str(len(v_loader)))\n",
        "     \n",
        "    valloader = data.DataLoader(\n",
        "            v_loader,\n",
        "            batch_size=1,\n",
        "            num_workers=16,\n",
        "        )\n",
        "    print(\"Val-Loader:\" +  str(len(valloader)))\n",
        "     \n",
        "    n_classes = t_loader.n_classes\n",
        "    print(\"N Classes: \" + str(n_classes))\n",
        "\n",
        "    return trainloader,valloader,n_classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soWxGnnlc7Sk",
        "colab_type": "text"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY1g2NoFdnJN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainModel(trainloader, valloader, model, train_iterations, val_interval, print_interval, num_classes, scheduler, optimizer, loss_fn, name):\n",
        "    val_loss_meter = averageMeter()\n",
        "    time_meter = averageMeter()\n",
        "    best_iou = -100.0\n",
        "    flag = True\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    running_metrics_val = runningScore(num_classes)\n",
        "    i = 0\n",
        "\n",
        "    while i <= train_iterations and flag:\n",
        "        for (images, labels) in trainloader:\n",
        "            i += 1\n",
        "            start_ts = time.time()\n",
        "            scheduler.step()\n",
        "            model.train()\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            loss = loss_fn(input=outputs, target=labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            time_meter.update(time.time() - start_ts)\n",
        "\n",
        "            if (i + 1) % print_interval == 0:\n",
        "                fmt_str = \"Iter [{:d}/{:d}]  Loss: {:.4f}  Time/Image: {:.4f}\"\n",
        "                print_str = fmt_str.format(\n",
        "                    i + 1,\n",
        "                    train_iterations,\n",
        "                    loss.item(),\n",
        "                    time_meter.avg / 1,\n",
        "                )\n",
        "\n",
        "                print(print_str)\n",
        "                logger.info(print_str)\n",
        "                writer.add_scalar(\"loss/train_loss\", loss.item(), i + 1)\n",
        "                time_meter.reset()\n",
        "\n",
        "            if (i + 1) % val_interval == 0 or (i + 1) == train_iterations:\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    for i_val, (images_val, labels_val) in tqdm(enumerate(valloader)):\n",
        "                        images_val = images_val.to(device)\n",
        "                        labels_val = labels_val.to(device)\n",
        "\n",
        "                        outputs = model(images_val)\n",
        "                        val_loss = loss_fn(input=outputs, target=labels_val)\n",
        "\n",
        "                        pred = outputs.data.max(1)[1].cpu().numpy()\n",
        "                        gt = labels_val.data.cpu().numpy()\n",
        "\n",
        "                        running_metrics_val.update(gt, pred)\n",
        "                        val_loss_meter.update(val_loss.item())\n",
        "\n",
        "                writer.add_scalar(\"loss/val_loss\", val_loss_meter.avg, i + 1)\n",
        "                logger.info(\"Iter %d Loss: %.4f\" % (i + 1, val_loss_meter.avg))\n",
        "\n",
        "                score, class_iou = running_metrics_val.get_scores()\n",
        "                for k, v in score.items():\n",
        "                    print(k, v)\n",
        "                    logger.info(\"{}: {}\".format(k, v))\n",
        "                    writer.add_scalar(\"val_metrics/{}\".format(k), v, i + 1)\n",
        "\n",
        "                for k, v in class_iou.items():\n",
        "                    logger.info(\"{}: {}\".format(k, v))\n",
        "                    writer.add_scalar(\"val_metrics/cls_{}\".format(k), v, i + 1)\n",
        "\n",
        "                val_loss_meter.reset()\n",
        "                running_metrics_val.reset()\n",
        "\n",
        "                if score[\"Mean IoU : \\t\"] >= best_iou:\n",
        "                    best_iou = score[\"Mean IoU : \\t\"]\n",
        "                    state = {\n",
        "                        \"epoch\": i + 1,\n",
        "                        \"model_state\": model.state_dict(),\n",
        "                        \"optimizer_state\": optimizer.state_dict(),\n",
        "                        \"scheduler_state\": scheduler.state_dict(),\n",
        "                        \"best_iou\": best_iou,\n",
        "                    }\n",
        "                    save_path = os.path.join(\n",
        "                        writer.file_writer.get_logdir(),\n",
        "                        \"{}_{}_best_model.pkl\".format(name,\"fcn8s\"),\n",
        "                    )\n",
        "                    torch.save(state, save_path)\n",
        "\n",
        "            if (i + 1) == train_iterations:\n",
        "                flag = False\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-5qI4Rvja8-",
        "colab_type": "text"
      },
      "source": [
        "# Running"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-3wpjlOjoHD",
        "colab_type": "text"
      },
      "source": [
        "## Adam Learning Rate Of .001 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUwMpNY_kBMG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_3_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_3_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNDXdiOpqXJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 250\n",
        "name = \"Adam_3\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_3_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7mhijpvj5Eq",
        "colab_type": "text"
      },
      "source": [
        "## Adam Learning Rate Of .0001\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTn5xhzVtzrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_4_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qQUdKW7rgrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adam_4\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGkJEzvsj1tQ",
        "colab_type": "text"
      },
      "source": [
        "## Adam Learning Rate Of .00001 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QD3dY8julbj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .00001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_5_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_5_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Po9bwJRlbes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 250\n",
        "name = \"Adam_5\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_5_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdWi6fAbl-XZ",
        "colab_type": "text"
      },
      "source": [
        "### View Adam Models in TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrTeovsymDCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na_JA68gmLtj",
        "colab_type": "text"
      },
      "source": [
        "## SGD Learning Rate of .001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oymP0zMgmREe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "SGD_3_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(SGD_3_Model, \"sgd\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6zBpW_Izc6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 250\n",
        "name = \"SGD_3\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, SGD_3_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TuAK7ACqzilw"
      },
      "source": [
        "## SGD Learning Rate of .0001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2RlC3mxJzilx",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "SGD_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(SGD_4_Model, \"sgd\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SVU5gFykzil2",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 250\n",
        "name = \"SGD_4\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, SGD_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r_-TRMJIzs2i"
      },
      "source": [
        "## SGD Learning Rate of .00001\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ikkgkug5zs2j",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .00001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "SGD_5_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(SGD_5_Model, \"sgd\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aZURj9lszs2m",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 250\n",
        "name = \"SGD_5\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, SGD_5_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YygrlRmpz6qp"
      },
      "source": [
        "### View SGD Models in TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aagJBnRLz6qr",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qmBN48qzz_Gm"
      },
      "source": [
        "## Adagrad Learning Rate of .001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3YXVo4__z_Gn",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adagrad_3_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adagrad_3_Model, \"adagrad\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fET7ORt7z_Gq",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 250\n",
        "name = \"Adagrad_3\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adagrad_3_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oT4eDLjAz_Gt"
      },
      "source": [
        "## Adagrad Learning Rate of .0001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TkTSrhwTz_Gt",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adagrad_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adagrad_4_Model, \"adagrad\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRFJF2MPz_Gw",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adagrad_4\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adagrad_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A30GakVsz_Gz"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "## Adagrad Learning Rate of .00001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uujiLpG4z_Gz",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .00001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adagrad_5_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adagrad_5_Model, \"adagrad\", loss, weight_decay)\n",
        "scheduler = get_scheduler(optimizer, None)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hHmZnislz_G2",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adagrad_5\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adagrad_5_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mf2q88UZz_G5"
      },
      "source": [
        "### View Adagrad Models in TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CU_JLKZ7z_G6",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvBzkxwq0w-Q",
        "colab_type": "text"
      },
      "source": [
        "## **Best Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNQElq9l01ct",
        "colab_type": "text"
      },
      "source": [
        "Best model is: ..... because ....\n",
        "\n",
        "Adam -4 we concluded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ROLITl5FofD",
        "colab_type": "text"
      },
      "source": [
        "### Different Schedulers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkRkHe-H5y88",
        "colab_type": "text"
      },
      "source": [
        "#### Adam(-4) No Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WrIc-JTO6Y5A",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_4_Model, \"adam\", loss, weight_decay)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7-YQt2yh6Y5E",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adam_4_No\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlPHAH7B57kE",
        "colab_type": "text"
      },
      "source": [
        "#### Adam(-4) Polynomial Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rhbXOCFW7rGJ",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_4_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = PolynomialLR(optimizer, 30000)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aE4aAHD87rGN",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adam_4_Poly\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vZoxHgPW-DK4"
      },
      "source": [
        "#### Adam(-4) MultiStepLR Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bUEPXlYO-DK6",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_4_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = ExponentialLR(optimizer, 0.95)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZhqvMW7e-DK9",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adam_4_Expo\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kfVwHMLW98lB"
      },
      "source": [
        "##### View Different Schedulers Models in TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OkTw6JMK98lD",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "075Wh4RBFPvK",
        "colab_type": "text"
      },
      "source": [
        "### Different Losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XpRoYO_F5Z1",
        "colab_type": "text"
      },
      "source": [
        "#### Adam(-4) cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s2hobvtHGJWr",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_4_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = PolynomialLR(optimizer, 30000)\n",
        "loss = get_loss_function(\"cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ej8B7XyJGJWt",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adam_4_Poly_Cross\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6mZggtYFmpP",
        "colab_type": "text"
      },
      "source": [
        "#### Adam(-4) bootstrapped_cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GDebvG_sFWH9",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_4_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = PolynomialLR(optimizer, 30000)\n",
        "loss = functools.partial(bootstrapped_cross_entropy2d, K=64*512)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jVTl3LiRFWIB",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adam_4_Poly_Boot\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KvOGaGAYGW1F"
      },
      "source": [
        "#### Adam(-4) multi_scale_cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2yM1om0GW1G",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_4_Model = torch.nn.DataParallel(fcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_4_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = PolynomialLR(optimizer, 30000)\n",
        "loss = get_loss_function(\"multi_scale_cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k_4KQd8wGW1J",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adam_4_Poly_Multi\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MXiFAunJGutj"
      },
      "source": [
        "##### View Different Losses Models in TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "emL3h6tSGutk",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn4E9c0g1tjz",
        "colab_type": "text"
      },
      "source": [
        "## Losing Weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzoIlNOW1zo6",
        "colab_type": "text"
      },
      "source": [
        "### Updated Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqRRuXqxnWBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class newfcn8s(nn.Module):\n",
        "    def __init__(self, n_classes=21, learned_billinear=True):\n",
        "        super(newfcn8s, self).__init__()\n",
        "        self.learned_billinear = False\n",
        "        self.n_classes = n_classes\n",
        "        self.loss = functools.partial(cross_entropy2d, size_average=False)\n",
        "\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=100),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 64, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 128, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        self.conv_block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        self.conv_block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, 3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        )\n",
        "\n",
        "        # self.conv_block5 = nn.Sequential(\n",
        "        #     nn.Conv2d(512, 512, 3, padding=1),\n",
        "        #     nn.ReLU(inplace=True),\n",
        "        #     nn.Conv2d(512, 512, 3, padding=1),\n",
        "        #     nn.ReLU(inplace=True),\n",
        "        #     nn.Conv2d(512, 512, 3, padding=1),\n",
        "        #     nn.ReLU(inplace=True),\n",
        "        #     nn.MaxPool2d(2, stride=2, ceil_mode=True),\n",
        "        # )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(512, 4096, 7),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(),\n",
        "            nn.Conv2d(4096, 4096, 1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout2d(),\n",
        "            nn.Conv2d(4096, self.n_classes, 1),\n",
        "        )\n",
        "\n",
        "        self.score_pool4 = nn.Conv2d(512, self.n_classes, 1)\n",
        "        self.score_pool3 = nn.Conv2d(256, self.n_classes, 1)\n",
        "\n",
        "        if self.learned_billinear:\n",
        "            self.upscore2 = nn.ConvTranspose2d(\n",
        "                self.n_classes, self.n_classes, 4, stride=2, bias=False\n",
        "            )\n",
        "            self.upscore4 = nn.ConvTranspose2d(\n",
        "                self.n_classes, self.n_classes, 4, stride=2, bias=False\n",
        "            )\n",
        "            self.upscore8 = nn.ConvTranspose2d(\n",
        "                self.n_classes, self.n_classes, 16, stride=8, bias=False\n",
        "            )\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.ConvTranspose2d):\n",
        "                m.weight.data.copy_(\n",
        "                    get_upsampling_weight(m.in_channels, m.out_channels, m.kernel_size[0])\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv_block1(x)\n",
        "        conv2 = self.conv_block2(conv1)\n",
        "        conv3 = self.conv_block3(conv2)\n",
        "        conv4 = self.conv_block4(conv3)\n",
        "        # conv5 = self.conv_block5(conv4)\n",
        "\n",
        "        score = self.classifier(conv4)\n",
        "\n",
        "        if self.learned_billinear:\n",
        "            upscore2 = self.upscore2(score)\n",
        "            score_pool4c = self.score_pool4(conv4)[\n",
        "                :, :, 5 : 5 + upscore2.size()[2], 5 : 5 + upscore2.size()[3]\n",
        "            ]\n",
        "            upscore_pool4 = self.upscore4(upscore2 + score_pool4c)\n",
        "\n",
        "            score_pool3c = self.score_pool3(conv3)[\n",
        "                :, :, 9 : 9 + upscore_pool4.size()[2], 9 : 9 + upscore_pool4.size()[3]\n",
        "            ]\n",
        "\n",
        "            out = self.upscore8(score_pool3c + upscore_pool4)[\n",
        "                :, :, 31 : 31 + x.size()[2], 31 : 31 + x.size()[3]\n",
        "            ]\n",
        "            return out.contiguous()\n",
        "\n",
        "        else:\n",
        "            score_pool4 = self.score_pool4(conv4)\n",
        "            score_pool3 = self.score_pool3(conv3)\n",
        "            score = F.upsample(score, score_pool4.size()[2:])\n",
        "            score += score_pool4\n",
        "            score = F.upsample(score, score_pool3.size()[2:])\n",
        "            score += score_pool3\n",
        "            out = F.upsample(score, x.size()[2:])\n",
        "\n",
        "        return out\n",
        "\n",
        "    def init_vgg16_params(self, vgg16, copy_fc8=True):\n",
        "        blocks = [\n",
        "            self.conv_block1,\n",
        "            self.conv_block2,\n",
        "            self.conv_block3,\n",
        "            self.conv_block4,\n",
        "            # self.conv_block5,\n",
        "        ]\n",
        "\n",
        "        ranges = [[0, 4], [5, 9], [10, 16], [17, 23], [24, 29]]\n",
        "        features = list(vgg16.features.children())\n",
        "\n",
        "        for idx, conv_block in enumerate(blocks):\n",
        "            for l1, l2 in zip(features[ranges[idx][0] : ranges[idx][1]], conv_block):\n",
        "                if isinstance(l1, nn.Conv2d) and isinstance(l2, nn.Conv2d):\n",
        "                    assert l1.weight.size() == l2.weight.size()\n",
        "                    assert l1.bias.size() == l2.bias.size()\n",
        "                    l2.weight.data = l1.weight.data\n",
        "                    l2.bias.data = l1.bias.data\n",
        "        for i1, i2 in zip([0, 3], [0, 3]):\n",
        "            l1 = vgg16.classifier[i1]\n",
        "            l2 = self.classifier[i2]\n",
        "            l2.weight.data = l1.weight.data.view(l2.weight.size())\n",
        "            l2.bias.data = l1.bias.data.view(l2.bias.size())\n",
        "        n_class = self.classifier[6].weight.size()[0]\n",
        "        if copy_fc8:\n",
        "            l1 = vgg16.classifier[6]\n",
        "            l2 = self.classifier[6]\n",
        "            l2.weight.data = l1.weight.data[:n_class, :].view(l2.weight.size())\n",
        "            l2.bias.data = l1.bias.data[:n_class]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NFAuNQ-d2LtP",
        "colab": {}
      },
      "source": [
        "image_size = (512,512)\n",
        "batch_size = 1\n",
        "num_workers = 16\n",
        "loss = .0001\n",
        "weight_decay = .0005\n",
        "data_path = '/content/VOCdevkit/VOC2012'\n",
        "\n",
        "trainloader,valloader,n_classes = setupLoaders(data_path, image_size, batch_size, num_workers)\n",
        "Adam_4_Model = torch.nn.DataParallel(newfcn8s(),device_ids=range(torch.cuda.device_count()))\n",
        "optimizer = get_optimizer(Adam_4_Model, \"adam\", loss, weight_decay)\n",
        "scheduler = PolynomialLR(optimizer, 30000)\n",
        "loss = get_loss_function(\"multi_scale_cross_entropy\", False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FQoZ0QOf2LtV",
        "colab": {}
      },
      "source": [
        "train_iterations = 30000\n",
        "val_interval = 5000\n",
        "print_interval = 500\n",
        "name = \"Adam_4_Lack_Block\"\n",
        "logs_base_dir = \"runs/\" + name\n",
        "writer = SummaryWriter(log_dir=logs_base_dir)\n",
        "\n",
        "trainModel(trainloader, valloader, Adam_4_Model, train_iterations, val_interval, print_interval, n_classes, scheduler, optimizer, loss, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TitpOcfu22H5"
      },
      "source": [
        "##### View Weight Loss Models in TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aT9XKlp_22H6",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=runs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbtvC0wQ245-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}